/**
 * GEMINI SERVICE - "Two-Pass" Architecture
 * 
 * Pass 1 (NEW): Audio â†’ Transcription (text with timestamps)
 * Pass 2: Video Frames + Text Context â†’ Scope Analysis
 * 
 * This solves the "Audio Deafness" problem where raw audio
 * gets only ~2% attention weight vs video frames.
 * By converting audio to text first, it becomes an INSTRUCTION
 * with proper weight in the generation process.
 */

import { GoogleGenAI, HarmCategory, HarmBlockThreshold } from "@google/genai";
import { rateLimiter } from "./rateLimiter";
import { transcribeAudio, transcribeMultipleAudio } from "./transcriptionService";
import { LineItem, RoomData, ProjectMetadata, ExtractedData, JobType } from "../types";
import { sanitizeLineItem, sortScopeItems } from "../utils/xactimateRules";
import { getSystemInstruction, buildUserPrompt } from "../utils/aiConfig";
import { enrichScopeWithLogistics } from "../utils/logisticsEngine";
import { sanitizeRooms } from "../utils/roomSanitizer";

const MODEL_NAME = "gemini-2.5-flash";
const API_KEY = process.env.API_KEY;

if (!API_KEY) {
  console.error("API Key is missing. Please check your .env file.");
}

// --- TEXT STREAM PARSER ---
const parseTextResponse = (text: string): { rooms: RoomData[], metadata: ProjectMetadata } => {
  const lines = text.split('\n').map(l => l.trim()).filter(l => l.length > 0);
  const rooms: RoomData[] = [];
  let currentRoom: RoomData | null = null;
  let metadata: ProjectMetadata = { loss_type_inference: "Unknown", severity_score: 5, confidence_level: "Low" };

  for (const line of lines) {
    try {
      if (line.startsWith('META::')) {
        const parts = line.substring(6).split('|');
        if (parts.length >= 3) {
          metadata = {
            loss_type_inference: parts[0].trim() || "Unknown",
            severity_score: parseInt(parts[1].trim()) || 5,
            confidence_level: (parts[2].trim() as any) || "Low"
          };
        }
      } else if (line.startsWith('ROOM::')) {
        const parts = line.substring(6).split('|');
        currentRoom = {
          id: crypto.randomUUID(),
          name: parts[0]?.trim() || "Unknown Room",
          timestamp_in: parts.length >= 3 ? parts[1]?.trim() : "",
          narrative_synthesis: parts.length >= 3 ? parts[2]?.trim() : (parts[1]?.trim() || ""),
          flagged_issues: [],
          items: []
        };
        rooms.push(currentRoom);
      } else if (line.startsWith('ITEM::')) {
        if (!currentRoom) continue;
        const parts = line.substring(6).split('|');
        if (parts.length < 5) continue; 
        
        currentRoom.items.push(sanitizeLineItem({
            id: crypto.randomUUID(),
            category: parts[0].trim(),
            selector: parts[1].trim(),
            code: `${parts[0].trim()} ${parts[1].trim()}`,
            description: parts[6]?.trim() || "Item description",
            activity: parts[2].trim(),
            quantity: parseFloat(parts[3].trim()) || 1,
            quantity_inference: parts[3].trim(),
            unit: parts[4].trim(),
            reasoning: parts[7]?.trim() || "Generated by AI",
            confidence: (parts[5]?.trim() as any) || 'Low'
        }));
      }
    } catch (e) {
      console.warn("Skipping malformed line:", line);
    }
  }
  rooms.forEach(r => r.items = sortScopeItems(r.items));
  return { rooms, metadata };
};

// --- HELPER: File to Base64 ---
const fileToBase64 = (file: File): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.readAsDataURL(file);
    reader.onload = () => resolve((reader.result as string).split(',')[1]);
    reader.onerror = error => reject(error);
  });
};

/**
 * MAIN FUNCTION: Two-Pass Scope Generation
 * 
 * Pass 1: Transcribe all audio sources to text
 * Pass 2: Analyze video frames with voice context injected as text
 */
export const generateScope = async (
  description: string,
  scopeContext: string,
  jobType: JobType,
  base64Images: string[] = [],
  videoData?: ExtractedData | null,
  audioFiles: File[] = [],
  onProgress?: (stage: string, detail?: string) => void
): Promise<{ rooms: RoomData[], metadata: ProjectMetadata }> => {
  return rateLimiter.enqueue(async () => {
    const ai = new GoogleGenAI({ apiKey: API_KEY });
    const scopePhase = jobType === 'R' ? 'reconstruction' : 'mitigation';
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // PASS 1: AUDIO TRANSCRIPTION (NEW)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    let voiceContext = "";
    
    onProgress?.("Transcribing audio", "Converting voice to text...");
    console.log("â•".repeat(60));
    console.log("  PASS 1: AUDIO TRANSCRIPTION");
    console.log("â•".repeat(60));
    
    try {
      // Collect all audio sources
      const audioSources: Array<{ base64: string; mimeType: string; label?: string }> = [];
      
      // Source 1: Video's embedded audio
      if (videoData?.audio) {
        audioSources.push({
          base64: videoData.audio,
          mimeType: "audio/wav",
          label: "Video Walkthrough"
        });
        console.log("ğŸ“¹ Found embedded video audio");
      }
      
      // Source 2: Separate audio files
      for (let i = 0; i < audioFiles.length; i++) {
        const audioFile = audioFiles[i];
        const base64Audio = await fileToBase64(audioFile);
        audioSources.push({
          base64: base64Audio,
          mimeType: audioFile.type || "audio/mp3",
          label: `Voice Note ${i + 1}`
        });
        console.log(`ğŸ¤ Found separate audio file: ${audioFile.name}`);
      }
      
      // Execute transcription
      if (audioSources.length > 0) {
        console.log(`ğŸ”„ Transcribing ${audioSources.length} audio source(s)...`);
        
        const transcriptionResult = audioSources.length === 1
          ? await transcribeAudio(audioSources[0].base64, audioSources[0].mimeType)
          : await transcribeMultipleAudio(audioSources);
        
        if (transcriptionResult.success) {
          voiceContext = transcriptionResult.formattedContext;
          console.log(`âœ… Transcription complete: ${transcriptionResult.segments.length} segments`);
          console.log("Preview:", transcriptionResult.rawTranscript.substring(0, 200) + "...");
        } else {
          console.warn("âš ï¸ Transcription returned empty - continuing without voice context");
        }
      } else {
        console.log("â„¹ï¸ No audio sources found - skipping Pass 1");
      }
      
    } catch (error) {
      // GRACEFUL DEGRADATION: If transcription fails, continue with visual analysis
      console.error("âŒ Pass 1 failed:", error);
      console.log("âš ï¸ Continuing with Pass 2 (visual only) - graceful degradation");
    }
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // PASS 2: VISUAL ANALYSIS WITH VOICE CONTEXT
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    onProgress?.("Analyzing scope", "Processing visual data with voice context...");
    console.log("\n" + "â•".repeat(60));
    console.log("  PASS 2: VISUAL ANALYSIS");
    console.log("â•".repeat(60));
    
    // Build the prompt with voice context INJECTED AS TEXT
    const basePrompt = buildUserPrompt(`${description} [Scope Context: ${scopeContext}]`, jobType);
    
    // The KEY change: Voice context goes BEFORE the main prompt
    // This gives it higher priority in the attention mechanism
    const fullPrompt = voiceContext 
      ? `${voiceContext}\n\n${basePrompt}`
      : basePrompt;
    
    if (voiceContext) {
      console.log("âœ… Voice context injected into prompt");
      console.log(`   Context length: ${voiceContext.length} characters`);
    }
    
    // Build parts array - VIDEO FRAMES + TEXT ONLY (no raw audio)
    const parts: any[] = [{ text: fullPrompt }];

    // Add Video Frames (NO AUDIO - it's already transcribed)
    if (videoData) {
      console.log(`ğŸ“¹ Adding ${videoData.frames.length} video frames`);
      videoData.frames.forEach(frame => {
        parts.push({ inlineData: { mimeType: "image/jpeg", data: frame } });
      });
    }

    // Add Photos
    if (base64Images.length > 0) {
      console.log(`ğŸ“¸ Adding ${base64Images.length} photos`);
      base64Images.forEach(img => {
        const mimeMatch = img.match(/^data:(image\/[a-zA-Z+]+);base64,/);
        const mimeType = mimeMatch ? mimeMatch[1] : "image/jpeg";
        const data = img.replace(/^data:image\/[a-zA-Z+]+;base64,/, "");
        parts.push({ inlineData: { mimeType, data } });
      });
    }

    // NOTE: We intentionally do NOT include raw audio here anymore
    // The voice information is now in `fullPrompt` as text

    try {
      console.log("ğŸš€ Sending to Gemini...");
      
      const response = await ai.models.generateContent({
        model: MODEL_NAME,
        contents: { parts },
        config: {
          systemInstruction: getSystemInstruction(scopePhase),
          responseMimeType: "text/plain",
          temperature: 0.1,
          maxOutputTokens: 8192,
          safetySettings: [
            { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_NONE },
          ]
        }
      });

      const candidate = response.candidates?.[0];
      let rawText = response.text || "";
      
      if ((!rawText || candidate?.finishReason === 'MAX_TOKENS') && candidate?.content?.parts) {
        rawText = candidate.content.parts.map((p: any) => p.text || '').join('');
      }
      if (!rawText) throw new Error("Empty response from AI.");

      console.log("âœ… Gemini response received");
      
      // Parse the response
      const parsedResult = parseTextResponse(rawText);

      // Apply Room Sanitizer (dedupe ghost rooms)
      const sanitized = sanitizeRooms(parsedResult.rooms);
      parsedResult.rooms = sanitized.rooms;
      
      if (sanitized.warnings.length > 0) {
        console.log("ğŸ§¹ Room Sanitizer:", sanitized.warnings);
      }
      if (sanitized.mergeCount > 0) {
        console.log(`âœ… Deduplicated ${sanitized.mergeCount} ghost room(s)`);
      }

      // Apply Logistics Engine
      parsedResult.rooms = enrichScopeWithLogistics(
        parsedResult.rooms, 
        parsedResult.metadata.severity_score, 
        scopeContext,
        parsedResult.metadata.loss_type_inference,
        jobType
      );

      console.log("\n" + "â•".repeat(60));
      console.log("  TWO-PASS COMPLETE");
      console.log("â•".repeat(60));
      console.log(`   Rooms: ${parsedResult.rooms.length}`);
      console.log(`   Total Items: ${parsedResult.rooms.reduce((sum, r) => sum + r.items.length, 0)}`);
      console.log(`   Voice Context: ${voiceContext ? 'YES' : 'NO'}`);
      
      return parsedResult;

    } catch (error) {
      console.error("âŒ Gemini Core Engine Error:", error);
      throw error;
    }
  });
};

/**
 * LEGACY EXPORT: For backwards compatibility
 * Some components might still import specific functions
 */
export { transcribeAudio, transcribeMultipleAudio };
